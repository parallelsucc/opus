{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1914464a-61be-4e92-b01d-8ee946a9a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91ec652-1d1e-47c9-a893-4b2b6eb85185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1131/3851827311.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/root/miniconda3/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-de-en/\"\n",
    "raw_datasets = load_dataset('wmt16/de-en')\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4bd2f9-847d-461d-ae62-c43653e7ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MarianConfig\n",
    "\n",
    "model_config = MarianConfig.from_pretrained(model_checkpoint)\n",
    "# ‰øÆÊîπÈÖçÁΩÆ\n",
    "model_config.output_hidden_states = True\n",
    "model_config.output_attentions = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21859eb4-c902-494a-ad02-0e04f78289d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# ÂèñÂá∫ÂêÑ‰∏™Â≠êÈõÜÁöÑ‰∏ÄÈÉ®ÂàÜÊï∞ÊçÆ\n",
    "train_subset = raw_datasets[\"train\"].select(range(1000000))\n",
    "validation_subset = raw_datasets[\"validation\"]\n",
    "test_subset = raw_datasets[\"test\"]\n",
    "\n",
    "# Â∞ÜËøô‰∫õÂ≠êÈõÜÁªÑÂêàÊàê‰∏Ä‰∏™Êñ∞ÁöÑ DatasetDict\n",
    "subset_datasets = {\"train\": train_subset, \"validation\": validation_subset, \"test\": test_subset}\n",
    "subset_dataset_dict = datasets.DatasetDict(subset_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c5628a-53d9-4907-9f13-54c5abaf53bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6142f392014a5ea05e926bbb2e6ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba8564640474eeb9a980e12414456ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99654996ba054985b61798ccb3e0c081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 16\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"de\"\n",
    "target_lang = \"en\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # model_inputs[\"labels_attention_mask\"] = labels[\"attention_mask\"]\n",
    "    return model_inputs\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    print('enter compute_metrics')\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print('out compute_metrics')\n",
    "    return result\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "# tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = subset_dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b0a3bc-5398-4963-b82a-446f47acdd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers.models.marian.modeling_marian as marian\n",
    "\n",
    "MarianDecoder = marian.MarianDecoder\n",
    "BaseModelOutputWithPastAndCrossAttentions = marian.BaseModelOutputWithPastAndCrossAttentions\n",
    "_prepare_4d_causal_attention_mask = marian._prepare_4d_causal_attention_mask\n",
    "_prepare_4d_attention_mask = marian._prepare_4d_attention_mask\n",
    "logger = marian.logger\n",
    "\n",
    "Optional = marian.Optional\n",
    "Union = marian.Union\n",
    "Tuple = marian.Tuple\n",
    "BaseModelOutput = marian.BaseModelOutput\n",
    "Seq2SeqModelOutput = marian.Seq2SeqModelOutput\n",
    "MarianPreTrainedModel = marian.MarianPreTrainedModel\n",
    "MarianEncoder = marian.MarianEncoder\n",
    "copy = marian.copy\n",
    "\n",
    "class CustomMarianDecoder(MarianDecoder):\n",
    "    def __init__(self, config: MarianConfig, embed_tokens):\n",
    "        super().__init__(config, embed_tokens)\n",
    "\n",
    "        # Ê∑ªÂä† encoder_layers ‰∏™ÂÖ®ËøûÊé•Â±Ç\n",
    "        self.additional_linear_layers = nn.ModuleList([\n",
    "            nn.Linear(config.hidden_size * config.encoder_layers, config.hidden_size, bias=False)\n",
    "            for _ in range(config.encoder_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    "\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "        )\n",
    "\n",
    "        # expand encoder attention mask\n",
    "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            encoder_attention_mask = _prepare_4d_attention_mask(\n",
    "                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
    "            )\n",
    "\n",
    "        # embed positions\n",
    "        positions = self.embed_positions(input_shape, past_key_values_length)\n",
    "\n",
    "        hidden_states = inputs_embeds + positions\n",
    "\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
    "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
    "            if attn_mask is not None:\n",
    "                assert attn_mask.size()[0] == (len(self.layers)), (\n",
    "                    f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        # ÂêàÂπ∂ encoder_hidden_states\n",
    "        # encoder_hidden_states[5] ÊòØ lash_hidden_state\n",
    "        encoder_hidden_states_all = torch.cat(encoder_hidden_states, dim=-1)\n",
    "        # print((encoder_hidden_states_all[:, :, :512] == encoder_hidden_states[5]).all())\n",
    "        # print((encoder_hidden_states_all[:, :, -512:] == encoder_hidden_states[5]).all())\n",
    "        # print((encoder_hidden_states[5] == torch.matmul(encoder_hidden_states_all, self.additional_linear_layers[0].weight.t())).all())\n",
    "        # print(encoder_hidden_states[5])\n",
    "        # print(self.additional_linear_layers[0].weight.t())\n",
    "        # print(torch.matmul(encoder_hidden_states_all, self.additional_linear_layers[0].weight.t()))\n",
    "        \n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "\n",
    "            # Ë∞ÉÁî®ÂØπÂ∫îÁöÑ linear_layer\n",
    "            linear_layer = self.additional_linear_layers[idx]\n",
    "            # Âä†ÊùÉÂ§ÑÁêÜ\n",
    "            encoder_hidden_states = linear_layer(encoder_hidden_states_all)\n",
    "            \n",
    "            # print(idx)\n",
    "            # print(encoder_hidden_states.shape)\n",
    "\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            if self.training:\n",
    "                dropout_probability = torch.rand([])\n",
    "                if dropout_probability < self.layerdrop:\n",
    "                    continue\n",
    "\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    head_mask[idx] if head_mask is not None else None,\n",
    "                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n",
    "                    None,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    cross_attn_layer_head_mask=(\n",
    "                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n",
    "                    ),\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "                if encoder_hidden_states is not None:\n",
    "                    all_cross_attentions += (layer_outputs[2],)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877a5906-7c56-4237-87ae-24a9bdcd1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSeq2SeqModel(MarianPreTrainedModel):\n",
    "    def __init__(self, config: MarianConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
    "\n",
    "        # We always use self.shared for token embeddings to ensure compatibility with all marian models\n",
    "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
    "        if self.config.share_encoder_decoder_embeddings:\n",
    "            encoder_embed_tokens = decoder_embed_tokens = self.shared\n",
    "        else:\n",
    "            # Since the embeddings are not shared, deepcopy the embeddings here for encoder\n",
    "            # and decoder to make sure they are not tied.\n",
    "            encoder_embed_tokens = copy.deepcopy(self.shared)\n",
    "            decoder_embed_tokens = copy.deepcopy(self.shared)\n",
    "            self.shared = None\n",
    "\n",
    "        self.encoder = MarianEncoder(config, encoder_embed_tokens)\n",
    "        # self.decoder = MarianDecoder(config, decoder_embed_tokens)\n",
    "        self.decoder = CustomMarianDecoder(config, decoder_embed_tokens)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        # This will return shared embeddings if they are shared else specific to encoder.\n",
    "        return self.get_encoder().get_input_embeddings()\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        if self.config.share_encoder_decoder_embeddings:\n",
    "            self.shared = value\n",
    "            self.encoder.embed_tokens = self.shared\n",
    "            self.decoder.embed_tokens = self.shared\n",
    "        else:  # if not shared only set encoder embeedings\n",
    "            self.encoder.embed_tokens = value\n",
    "\n",
    "    def get_decoder_input_embeddings(self):\n",
    "        if self.config.share_encoder_decoder_embeddings:\n",
    "            raise ValueError(\n",
    "                \"`get_decoder_input_embeddings` should not be called if `config.share_encoder_decoder_embeddings` \"\n",
    "                \"is `True`. Please use `get_input_embeddings` instead.\"\n",
    "            )\n",
    "        return self.get_decoder().get_input_embeddings()\n",
    "\n",
    "    def set_decoder_input_embeddings(self, value):\n",
    "        if self.config.share_encoder_decoder_embeddings:\n",
    "            raise ValueError(\n",
    "                \"`config.share_encoder_decoder_embeddings` is set to `True` meaning the decoder input embeddings \"\n",
    "                \"are shared with the encoder. In order to set the decoder input embeddings, you should simply set \"\n",
    "                \"the encoder input embeddings by calling `set_input_embeddings` with the appropriate embeddings.\"\n",
    "            )\n",
    "        self.decoder.embed_tokens = value\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def resize_decoder_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
    "        if self.config.share_encoder_decoder_embeddings:\n",
    "            raise ValueError(\n",
    "                \"`resize_decoder_token_embeddings` should not be called if `config.share_encoder_decoder_embeddings` \"\n",
    "                \"is `True`. Please use `resize_token_embeddings` instead.\"\n",
    "            )\n",
    "\n",
    "        old_embeddings = self.get_decoder_input_embeddings()\n",
    "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
    "        self.set_decoder_input_embeddings(new_embeddings)\n",
    "\n",
    "        model_embeds = self.get_decoder_input_embeddings()\n",
    "\n",
    "        if new_num_tokens is None:\n",
    "            return model_embeds\n",
    "\n",
    "        # Update base model and current model config\n",
    "        self.config.decoder_vocab_size = new_num_tokens\n",
    "\n",
    "        # Tie weights again if needed\n",
    "        self.tie_weights()\n",
    "\n",
    "        return model_embeds\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Union[Tuple[torch.Tensor], BaseModelOutput]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Seq2SeqModelOutput:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        modified = False\n",
    "        if encoder_outputs is not None:\n",
    "            modified = True\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        if modified:\n",
    "            encoder_outputs_modified = tuple(tensor.repeat_interleave(4, dim=0) for tensor in encoder_outputs[1])\n",
    "        else :\n",
    "            encoder_outputs_modified = encoder_outputs[1]\n",
    "\n",
    "        # print((encoder_outputs_modified[6] == encoder_outputs[0]).all())\n",
    "\n",
    "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_outputs_modified[1:],\n",
    "            # encoder_hidden_states=encoder_outputs[0],\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs + encoder_outputs\n",
    "\n",
    "        return Seq2SeqModelOutput(\n",
    "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06aa2bc5-21fe-492b-8385-8634da981506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomSeq2SeqModel were not initialized from the model checkpoint at Helsinki-NLP/opus-mt-de-en/ and are newly initialized: ['model.decoder.additional_linear_layers.0.weight', 'model.decoder.additional_linear_layers.1.weight', 'model.decoder.additional_linear_layers.2.weight', 'model.decoder.additional_linear_layers.3.weight', 'model.decoder.additional_linear_layers.4.weight', 'model.decoder.additional_linear_layers.5.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, config=model_config)\n",
    "model.model = CustomSeq2SeqModel.from_pretrained(model_checkpoint, config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0151dc5e-437a-430a-a4b0-8a8ec43a0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.model.decoder.additional_linear_layers.named_parameters():\n",
    "    init.constant_(param, 0.)\n",
    "    init.eye_(param[:, -model.config.hidden_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4d7355a-4103-4267-933f-973b83c18914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(10000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10))\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-translation\",\n",
    "    evaluation_strategy = \"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    # train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    train_dataset=small_train_dataset,\n",
    "    # eval_dataset=small_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d98ef677-009a-42af-a34d-52c7e10a867b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.141500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[58100]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter compute_metrics\n",
      "out compute_metrics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1737123727798462,\n",
       " 'eval_bleu': 37.9373,\n",
       " 'eval_gen_len': 25.5815,\n",
       " 'eval_runtime': 93.9292,\n",
       " 'eval_samples_per_second': 31.928,\n",
       " 'eval_steps_per_second': 2.002,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate(output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d173650-a901-4c61-b3de-7b15e4f95822",
   "metadata": {},
   "outputs": [],
   "source": [
    "40.1989,\n",
    "\n",
    "1000, 39.4744,\n",
    "10000, 38.4184,\n",
    "10000, 37.9373,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
